# Data Analytics - SQL Portfolio

Welcome to the SQL portion of my portfolio!

This code repository contains examples of SQL I've written that showcases a variety of use cases and real world examples. Projects range from looking at Superbowl statistics to store analytics to customer and order information, and each project utilizes vital SQL functionalities to achieve the goal of analyzing data in efficient and meaningful ways.

## Highlighted Projects - Extended Descriptions

+ **[Data Cleaning](https://github.com/MajorlyData/Data-Analytics-Portfolio/blob/main/SQL/Data%20Cleaning%20and%20Exploratory%20Data%20Analysis%20Pt.%201) and [Exploratory Data Analysis](https://github.com/MajorlyData/Data-Analytics-Portfolio/blob/main/SQL/Data%20Cleaning%20and%20Exploratory%20Data%20Analysis%20Pt.%202):** In this 2-part project, I performed data cleaning and exploratory data analysis on a dataset to gather valuable insight into utilizing real-world strategies to clean, explore and analyze data from company layoff information. First, I removed duplicates from the data by creating a staging table and using the row_number() window function to figure out which rows were exactly the same. I then standardized the data, fixing any obvious errors in each column and using contextual information to fill in the blank or NULL values. Next, I fixed any additional NULL or blank values, finally removing any unnecessary data not useful to the defined scope of the project. After cleaning the data, I perfomed Exploratory Data Analysis by taking a deeper dive into what the data was showing. To do this, I used a variety of queries, each attempting to uncover useful information that the data provided, such as looking at the total number of employees laid off by year or industry and ranking each company by total laid off partitioned over the year itself.
+ **[Customer & Order Analytics](https://github.com/MajorlyData/Data-Analytics-Portfolio/blob/main/SQL/Customer%20%26%20Order%20Analytics):** For this project, I queried multiple instances of data from a database named Sales_DB to highlight a variety of real world observations, including showing the number of orders for a specific month, the types of orders placed and the amount of revenue made. I also utilized important SQL functions to answer vital questions about the data, such as using JOINs to connect two tables and extract overlapping information, using subqueries to check if values match each other and making use of aggregate functions to evaluate different points of interest, such as SUM and COUNT. Lastly, and importantly, I employed data cleanup to make sure the data I did extrapolate was correct and in good standing, specifically to filter out any orderID or Product that may have been entered incorrectly or have become corrupted by external factors.
+ **[Bike Lane Analytics](https://github.com/MajorlyData/Data-Analytics-Portfolio/blob/main/SQL/Bike%20Lane%20Analytics):** In this project, I used SQL to query and analyze a table containing information about bike lanes around a city. The table included information such as each lane's street name, safety rating and the year it was installed. Making use of vital SQL functions such as CTEs and Window Functions, including 'partition by' and 'rank()', I discovered valuable insights into each bike lane and its standing within the recommendations of the city. For example, I first gathered each bike lane's average safety rating, as each one has two separate ratings. Using that value, I employed the 'partition by' clause to show each lane's average safety rating while still displaying each bike lane separately. Finally, I used a 'case' statement to gauge whether each bike lane was good, bad or needed to be improved, thus creating a resulting table that can easily be understood and read over efficiently.
+ **[Sales Analytics and Insights](https://github.com/MajorlyData/Data-Analytics-Portfolio/blob/main/SQL/Sales%20Analytics%20and%20Insights):** This project pulls from the Chinook database, a set of tables that includes data such as customer information, sales info and details of products sold, which in this case are tracks and albums. Using extensive data analysis through SQL functionalities, I delved deep into creating connections between the data to pull valuable insights from the dataset. For example, I took a look at where customers are from, specifically listing all the customers from Brazil and their invoice information. Likewise, I analyzed employee information in the same light, except rather than country, I looked at the invoices actually associated with each sales agent, showing which agent made which sale. Lastly, I looked at sales amounts in the year 2009, and discovered which agent made the most in sales in that year. Throughout the project, I used a multitude of SQL functions, including concatenating strings, using LIKE to find relevant matches in the dataset and utilizing multiple joins to connect more than two tables.
